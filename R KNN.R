##### KNN 알고리즘 : 최근접 이웃 분류 #####
# 가장 단순한 머신러닝 알고리즘 중 하나지만, 광범위하게 사용된다.
# 레이블이 없는 예시를 레이블된 유사한 예시의 클래스로 할당해 분류한다.
# 일반적으로 특징과 목표 클래스 간에 관계가 많고 복잡하거나 이해하기 매우 어렵지만, 
# 유사한 클래스 유형의 아이템들이 상당히 동질적인 경향을 띤다면 분류 작업에 KNN이 제격이다.

## 컴퓨터 비전 응용 (정지 영상 및 동영상에서 광학 글자 인식과 얼굴 인식)
## 영화나 음악 추천에 대한 개인별 선호 예측
## 특정 단백질 및 질병 발견에 사용 가능한 유전자 데이터의 패턴 인식

# 그러나 데이터에 잡음이 많고 그룹 간에 명확한 구분이 없다면 최근접 이웃 알고리즘은 클래스 경계를 식별하기 위해 어려움을 겪는다.

# 장점
## 단순하고 효율적이다.
## 기저 데이터 분포에 대한 가정을 하지 않는다.
## 훈련 단계가 빠르다.

# 단점
## 모델을 생성하지 않아 특징과 클래스 간의 관계를 이해하는 능력이 제약된다.
## 적절한 k의 선택이 필요하다.
## 분류 단계가 느리다
## 명목 특징 및 누락 데이터를 위한 추가 처리가 필요하다.

##### 거리로 유사도 측정 #####
# 최근접 이웃을 찾으려면 거리 함수(distance function)나 두 인스턴ㅅ 간의 유사도를 측정하는 공식이 필요하다
# 전통적으로 knn 알고리즘은 유사도를 계산하는데 유클리드 거리(euclidean distance)를 사용한다.

##### 적절한 k 선택 #####
# knn에 사용할 이웃의 개수는 모델이 미래 데이터에 대해 일반화되는 능력을 결정한다.
# k=n일 때 n개의 최근접 이웃에서 다수의 클래스에 분류된다.
# 훈련데이터에 대한 과적합(overfitting)과 과소적합(underfitting) 사이의 균형은 편향 분산 트레이드 오프(bias-variance tradeoff)로 알려진 문제이다.
# k를 큰 값으로 선택하면 잡음이 많은 데이터로 인한 영향이나 분산은 감소하지만, 작더라도 중요한 패턴을 무시하는 위험을 감수하는 학습자로 편향될 수 있다.
# 모든 훈련 인스턴스가 최종 투표에 나타나기 때문에 가장 일반적인 클래스는 항상 이 투표자들의 대다수이다. 
# 결과적으로 모델은 최근접 이웃과 상관없이 항상 대다수의 클래스를 예측한다.

# 정반대로 한 개의 최근접 이웃을 사용할 경우에는 잡음이 있는 데이터나 이상치가 예시의 분류에 과도한 영향을 미친다. 
# k 값이 작을수록 더 복잡한 결정 경계가 만들어져 훈련 데이터에 세밀하게 맞춰진다.

# 최적의 k 값은 두 극단 사이 어딘가에 있다.
# 실제 k의 선택은 학습될 개념의 난이도와 훈련 데이터의 레코드 개수에 의존한다.
# 보통 관례적으로 훈련 예시 개수의 제곱근으로 두고 시작한다.
# 그러나 다양한 데이터셋에 대해 몇 개의 k 값을 테스트해 분류 성능이 가장 좋은 것을 선택하는 방식이 좋다.
# 이때 데이터에 잡음이 많지 않고 훈련 데이터셋이 클 경우 k의 선택이 덜 중요해진다.

##### knn 사용을 위한 데이터 준비 #####
# 일반적으로 특징은 knn 알고리즘에 적용하기 전에 표준 범위롤 변환된다.
# 이유 : 거리 공식이 특정한 측정 방법에 매우 의존적이기 때문이다.
# 특정한 특징 값의 범위가 다른 특징에 비해 매우 크다면 거리 측정치는 큰 범위를 갖는 특징에 따라 크게 좌우된다.
# 따라서 특징을 재조정해 각 특징이 거리 공식에 상대적으로 통일하게 기여하게 범위를 줄이거나 늘리는 것

# knn을 위해 특징을 재조정하는 전통적인 방법은 최소-최대 정규화(min-max normalization)이다.
# 다른 일반적인 변환을 z-점수 표준화(z-score standardization)이라고 한다.

# 유클리드 거리 공식은 명목 데이터에는 정의되지 않는다.
# 명목 특징 간에 거리를 계산하기 위해 특징을 수치 형식으로 변환해야 한다.
# 대표적인 방법은 더미 코딩(dummy coding)을 활용하는 것이다.
# 더미 코딩의 편리한 면은 최소-최대 정규화가 적용된 수치 데이터와 동일한 범위에 속한다.
# 추가 변환이 필요 없다.

##### knn 알고리즘이 게으른 이유 #####
# 최근접 이웃 방법에 기반을 둔 분류 알고리즘은 게으른 학습(lazy learning)이라고 한다.
# 엄밀히 말하면 추상화가 일어나지 않기 때문이다.
# 추상화 및 일반화 단계가 함께 생략되고, 이로 인해 학습의 정의가 약화된다.

# 따라서 훈련 단계가 매우 빠르게 일어나게 되는데, 실제 훈련 단계에서는 아무것도 훈련하지 않는다.
# 단점은 예측 단계가 훈련 단계에 비해 상대적으로 느린 경향이 있다는 것이다.
# 추상화된 모델보다 훈련 인스턴스에 많이 의존하기 때문에 게으른 학습은 인스턴스 기반 학습(instance based learning) 또는 암기 학습(rote learning)이라고도 한다.

# 인스턴스 기반 학습자는 모델을 만들지 않기 때문에, 비모수 학습(non-parametric) 방법의 부류라고 말할 수 있다,
# 비모수 방법은 기저 데이터 대해 이론을 생성하지 않기 때문에 분류기의 데이터 이용 방법을 이해하기 어렵다.
# 한편 이방법에서 학습자는 사전에 가정돼 있는 잠재적으로 편향되어 있는 함수 형태에 데이터를 맞추기 보다 자연스러운 패턴을 찾는다.

# knn 분류기가 게으른 것으로 간주되지만, 꽤 강력하다.
# knn은 아주 단순한 알고리즘이지만, 극단적으로 복잡한 작업을 대처하는 데 적합하다.



##### 데이터 예제 #####
# 머신러닝이 분류를 자동화하면 00시스템에 상당한 혜택을 줄 수 있다.
# 과정이 자동화되면서 검출 단계의 효율이 향상돼 담당자가 진단 시간은 적게, 치료 시간은 길게 쓸 수 있다.
# 자동화된 분류 시스템은 검사 단계에서 근원적으로 주관적인 사람 요소를 없애고, 좀 더 높은 정확성으로 검출한다.

# 경로 설정
getwd()
setwd("C:/Users/sypar/Desktop/git/데이터_실습/Machine Learning with R (2nd Ed.)/Chapter 03")

# 1단계 : 데이터 수집
# 2단계 : 데이터 탐색과 준비
# 3단계 : 데이터 모델로 훈련
# 4단계 : 모델 성능 평가
# 5단계 : 모델 성능 개선선
