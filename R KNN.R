#### KNN 알고리즘 : 최근접 이웃 분류 #####
# 가장 단순한 머신러닝 알고리즘 중 하나지만, 광범위하게 사용된다.
# 레이블이 없는 예시를 레이블된 유사한 예시의 클래스로 할당해 분류한다.
# 일반적으로 특징과 목표 클래스 간에 관계가 많고 복잡하거나 이해하기 매우 어렵지만, 
# 유사한 클래스 유형의 아이템들이 상당히 동질적인 경향을 띤다면 분류 작업에 KNN이 제격이다.

## 컴퓨터 비전 응용 (정지 영상 및 동영상에서 광학 글자 인식과 얼굴 인식)
## 영화나 음악 추천에 대한 개인별 선호 예측
## 특정 단백질 및 질병 발견에 사용 가능한 유전자 데이터의 패턴 인식

# 그러나 데이터에 잡음이 많고 그룹 간에 명확한 구분이 없다면 최근접 이웃 알고리즘은 클래스 경계를 식별하기 위해 어려움을 겪는다.

# 장점
## 단순하고 효율적이다.
## 기저 데이터 분포에 대한 가정을 하지 않는다.
## 훈련 단계가 빠르다.

# 단점
## 모델을 생성하지 않아 특징과 클래스 간의 관계를 이해하는 능력이 제약된다.
## 적절한 k의 선택이 필요하다.
## 분류 단계가 느리다
## 명목 특징 및 누락 데이터를 위한 추가 처리가 필요하다.

###### 거리로 유사도 측정 #####
# 최근접 이웃을 찾으려면 거리 함수(distance function)나 두 인스턴ㅅ 간의 유사도를 측정하는 공식이 필요하다
# 전통적으로 knn 알고리즘은 유사도를 계산하는데 유클리드 거리(euclidean distance)를 사용한다.

##### 적절한 k 선택 #####
# knn에 사용할 이웃의 개수는 모델이 미래 데이터에 대해 일반화되는 능력을 결정한다.
# k=n일 때 n개의 최근접 이웃에서 다수의 클래스에 분류된다.
# 훈련데이터에 대한 과적합(overfitting)과 과소적합(underfitting) 사이의 균형은 편향 분산 트레이드 오프(bias-variance tradeoff)로 알려진 문제이다.
# k를 큰 값으로 선택하면 잡음이 많은 데이터로 인한 영향이나 분산은 감소하지만, 작더라도 중요한 패턴을 무시하는 위험을 감수하는 학습자로 편향될 수 있다.
# 모든 훈련 인스턴스가 최종 투표에 나타나기 때문에 가장 일반적인 클래스는 항상 이 투표자들의 대다수이다. 
# 결과적으로 모델은 최근접 이웃과 상관없이 항상 대다수의 클래스를 예측한다.

# 정반대로 한 개의 최근접 이웃을 사용할 경우에는 잡음이 있는 데이터나 이상치가 예시의 분류에 과도한 영향을 미친다. 
# k 값이 작을수록 더 복잡한 결정 경계가 만들어져 훈련 데이터에 세밀하게 맞춰진다.

# 최적의 k 값은 두 극단 사이 어딘가에 있다.
# 실제 k의 선택은 학습될 개념의 난이도와 훈련 데이터의 레코드 개수에 의존한다.
# 보통 관례적으로 훈련 예시 개수의 제곱근으로 두고 시작한다.
# 그러나 다양한 데이터셋에 대해 몇 개의 k 값을 테스트해 분류 성능이 가장 좋은 것을 선택하는 방식이 좋다.
# 이때 데이터에 잡음이 많지 않고 훈련 데이터셋이 클 경우 k의 선택이 덜 중요해진다.

###### knn 사용을 위한 데이터 준비 #####
# 일반적으로 특징은 knn 알고리즘에 적용하기 전에 표준 범위롤 변환된다.
# 이유 : 거리 공식이 특정한 측정 방법에 매우 의존적이기 때문이다.
# 특정한 특징 값의 범위가 다른 특징에 비해 매우 크다면 거리 측정치는 큰 범위를 갖는 특징에 따라 크게 좌우된다.
# 따라서 특징을 재조정해 각 특징이 거리 공식에 상대적으로 통일하게 기여하게 범위를 줄이거나 늘리는 것

# knn을 위해 특징을 재조정하는 전통적인 방법은 최소-최대 정규화(min-max normalization)


