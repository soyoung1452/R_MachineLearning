##### KNN 알고리즘 : 최근접 이웃 분류 #####
# 가장 단순한 머신러닝 알고리즘 중 하나지만, 광범위하게 사용된다.
# 레이블이 없는 예시를 레이블된 유사한 예시의 클래스로 할당해 분류한다.
# 일반적으로 특징과 목표 클래스 간에 관계가 많고 복잡하거나 이해하기 매우 어렵지만, 
# 유사한 클래스 유형의 아이템들이 상당히 동질적인 경향을 띤다면 분류 작업에 KNN이 제격이다.

## 컴퓨터 비전 응용 (정지 영상 및 동영상에서 광학 글자 인식과 얼굴 인식)
## 영화나 음악 추천에 대한 개인별 선호 예측
## 특정 단백질 및 질병 발견에 사용 가능한 유전자 데이터의 패턴 인식

# 그러나 데이터에 잡음이 많고 그룹 간에 명확한 구분이 없다면 최근접 이웃 알고리즘은 클래스 경계를 식별하기 위해 어려움을 겪는다.

# 장점
## 단순하고 효율적이다.
## 기저 데이터 분포에 대한 가정을 하지 않는다.
## 훈련 단계가 빠르다.

# 단점
## 모델을 생성하지 않아 특징과 클래스 간의 관계를 이해하는 능력이 제약된다.
## 적절한 k의 선택이 필요하다.
## 분류 단계가 느리다
## 명목 특징 및 누락 데이터를 위한 추가 처리가 필요하다.

##### 거리로 유사도 측정 #####
# 최근접 이웃을 찾으려면 거리 함수(distance function)나 두 인스턴ㅅ 간의 유사도를 측정하는 공식이 필요하다
# 전통적으로 knn 알고리즘은 유사도를 계산하는데 유클리드 거리(euclidean distance)를 사용한다.

##### 적절한 k 선택 #####
# knn에 사용할 이웃의 개수는 모델이 미래 데이터에 대해 일반화되는 능력을 결정한다.
# k=n일 때 n개의 최근접 이웃에서 다수의 클래스에 분류된다.
# 훈련데이터에 대한 과적합(overfitting)과 과소적합(underfitting) 사이의 균형은 편향 분산 트레이드 오프(bias-variance tradeoff)로 알려진 문제이다.
# k를 큰 값으로 선택하면 잡음이 많은 데이터로 인한 영향이나 분산은 감소하지만, 작더라도 중요한 패턴을 무시하는 위험을 감수하는 학습자로 편향될 수 있다.
# 모든 훈련 인스턴스가 최종 투표에 나타나기 때문에 가장 일반적인 클래스는 항상 이 투표자들의 대다수이다. 
# 결과적으로 모델은 최근접 이웃과 상관없이 항상 대다수의 클래스를 예측한다.

# 정반대로 한 개의 최근접 이웃을 사용할 경우에는 잡음이 있는 데이터나 이상치가 예시의 분류에 과도한 영향을 미친다. 
# k 값이 작을수록 더 복잡한 결정 경계가 만들어져 훈련 데이터에 세밀하게 맞춰진다.

# 최적의 k 값은 두 극단 사이 어딘가에 있다.
# 실제 k의 선택은 학습될 개념의 난이도와 훈련 데이터의 레코드 개수에 의존한다.
# 보통 관례적으로 훈련 예시 개수의 제곱근으로 두고 시작한다.
# 그러나 다양한 데이터셋에 대해 몇 개의 k 값을 테스트해 분류 성능이 가장 좋은 것을 선택하는 방식이 좋다.
# 이때 데이터에 잡음이 많지 않고 훈련 데이터셋이 클 경우 k의 선택이 덜 중요해진다.

##### knn 사용을 위한 데이터 준비 #####
# 일반적으로 특징은 knn 알고리즘에 적용하기 전에 표준 범위롤 변환된다.
# 이유 : 거리 공식이 특정한 측정 방법에 매우 의존적이기 때문이다.
# 특정한 특징 값의 범위가 다른 특징에 비해 매우 크다면 거리 측정치는 큰 범위를 갖는 특징에 따라 크게 좌우된다.
# 따라서 특징을 재조정해 각 특징이 거리 공식에 상대적으로 통일하게 기여하게 범위를 줄이거나 늘리는 것

# knn을 위해 특징을 재조정하는 전통적인 방법은 최소-최대 정규화(min-max normalization)이다.
# 다른 일반적인 변환을 z-점수 표준화(z-score standardization)이라고 한다.

# 유클리드 거리 공식은 명목 데이터에는 정의되지 않는다.
# 명목 특징 간에 거리를 계산하기 위해 특징을 수치 형식으로 변환해야 한다.
# 대표적인 방법은 더미 코딩(dummy coding)을 활용하는 것이다.
# 더미 코딩의 편리한 면은 최소-최대 정규화가 적용된 수치 데이터와 동일한 범위에 속한다.
# 추가 변환이 필요 없다.

##### knn 알고리즘이 게으른 이유 #####
# 최근접 이웃 방법에 기반을 둔 분류 알고리즘은 게으른 학습(lazy learning)이라고 한다.
# 엄밀히 말하면 추상화가 일어나지 않기 때문이다.
# 추상화 및 일반화 단계가 함께 생략되고, 이로 인해 학습의 정의가 약화된다.

# 따라서 훈련 단계가 매우 빠르게 일어나게 되는데, 실제 훈련 단계에서는 아무것도 훈련하지 않는다.
# 단점은 예측 단계가 훈련 단계에 비해 상대적으로 느린 경향이 있다는 것이다.
# 추상화된 모델보다 훈련 인스턴스에 많이 의존하기 때문에 게으른 학습은 인스턴스 기반 학습(instance based learning) 또는 암기 학습(rote learning)이라고도 한다.

# 인스턴스 기반 학습자는 모델을 만들지 않기 때문에, 비모수 학습(non-parametric) 방법의 부류라고 말할 수 있다,
# 비모수 방법은 기저 데이터 대해 이론을 생성하지 않기 때문에 분류기의 데이터 이용 방법을 이해하기 어렵다.
# 한편 이방법에서 학습자는 사전에 가정돼 있는 잠재적으로 편향되어 있는 함수 형태에 데이터를 맞추기 보다 자연스러운 패턴을 찾는다.

# knn 분류기가 게으른 것으로 간주되지만, 꽤 강력하다.
# knn은 아주 단순한 알고리즘이지만, 극단적으로 복잡한 작업을 대처하는 데 적합하다.



##### 데이터 예제 #####
# 머신러닝이 분류를 자동화하면 00시스템에 상당한 혜택을 줄 수 있다.
# 과정이 자동화되면서 검출 단계의 효율이 향상돼 담당자가 진단 시간은 적게, 치료 시간은 길게 쓸 수 있다.
# 자동화된 분류 시스템은 검사 단계에서 근원적으로 주관적인 사람 요소를 없애고, 좀 더 높은 정확성으로 검출한다.

# 경로 설정
getwd()
setwd("C:/Users/sypar/Desktop/git/데이터_실습/Machine Learning with R (2nd Ed.)/Chapter 03")

# 1단계 : 데이터 수집
# csv 파일 불러오기
data <- read.csv("wisc_bc_data.csv", stringsAsFactors = F)

# 2단계 : 데이터 탐색과 준비
# 데이터 구조 (observation/variable 갯수 확인 및 변수 타입 확인)
str(data)
# 불필요한 데이터 삭제
data <- data[-1]  ## 첫번째 변수 삭제제
# 특정 변수의 빈도 확인
table(data$diagnosis)
# 데이터 타입 변경
# 대부분 R 머신러닝 분류기의 경우 target 변수가 factor로 코딩되어야 한다.
data$diagnosis <- factor(data$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
# 특정 변수의 비율 확인
round(prop.table(table(data$diagnosis))*100, digits = 1)
# 변수 요약 통계량
summary(data)
# 변환 : 수치 데이터 정규화
# 정규화 함수
normalize <- function(x){
  return ((x - min(x))/(max(x) - min(x)))
}
# lapply() : 각 리스트 항목에 지정된 함수를 적용 -> 리스트 반환
data_normal <- as.data.frame(lapply(data[2:31], normalize))
# 데이터 준비 : 훈련 및 테스트 데이터셋 생성
# 무작위로 생성된 데이터 셋의 경우
data_train <- data_normal[1:469, ]
data_test <- data_normal[470:569, ]
# 타켓 변수 데이트 세트 생성
data_train_labels <- data[1:469, 1]
data_test_labels <- data[470:569, 1]

# 3단계 : 데이터 모델로 훈련
# knn 알고리즘은 훈련 단계에서 모델을 실제로 구축하지 않는다. 
# 게으른 학습자를 훈련하는 과정은 단순히 입력 데이터를 구조화된 형식으로 저장하는 것이다.
# class : 분류를 위한 기본 R 함수들을 제공하는 패키지
install.packages("class")
library(class)
# md <- knn(train, test, class, k)
# 469 제곱근인 21로 k값 설정
data_md_pred <- knn(train = data_train, test = data_test, cl = data_train_labels, k = 21)

# 4단계 : 모델 성능 평가
# data_md_pred 벡터에 있는 예측된 클래스가 data_test_lables 벡터에 있는 알려진 값과 얼마나 일치하는가를 평가하는 것
install.packages("gmodels")
library(gmodels)
CrossTable(x = data_test_labels, y = data_md_pred, prop.chisq = F)   # 카이제곱값 출력 X


# 5단계 : 모델 성능 개선
# 수치 특징 재조정
# 변환 : z-점수 표준화 (내장함수 scale())
# 전통적으로 normalization이 knn 분류기에 사용
# 이상치에 좀 더 큰 가중치를 주는 것이 합리적인 경우 z-점수 표준화가  예측 정확성을 향상할 수 있을지 확인해보는 것 이 좋다
data_z <- as.data.frame(scale(data[-1]))
# 분석 수행
data_train <- data_z[1:469, ]; data_train_labels <- data[1:469, 1]
data_test <- data_z[470:569, ]; data_test_labels <- data[470:569, 1]
data_md_pred <- knn(data_train, data_test, data_train_labels, k = 21)
CrossTable(data_test_labels, data_md_pred, prop.chisq = F)
# k 대체 값 테스트
# 다양한 k값으로 성능을 검사해 더 좋은 결과를 얻을 수 있다.






